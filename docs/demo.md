# Crux MVP Demo Script (~3 minutes)

## The Problem (0:00–0:20)

> You want to understand whether AI will replace software engineers. So you read Karpathy's tweets, LeCun's rebuttals, Garry Tan's threads. Two hours later you have a pile of opinions — but you still can't answer the basic question: **where exactly do they disagree, and what evidence would change their minds?**
>
> That's the problem. The internet has infinite opinions and zero structured disagreement.

## What Crux Is (0:20–0:45)

> Crux is a debate engine. You pick a topic, pick the voices you care about — real people like Karpathy, Yann LeCun, Michael Saylor, Cathie Wood — and Crux spins up AI agents grounded in their actual published positions from Twitter and Substack.
>
> Then they debate. Not to reach consensus — to find the **crux**: the core assumptions driving the disagreement, and the **flip conditions** — the specific evidence that would actually change each side's mind.

## Live Demo — Setup (0:45–1:15)

*[Screen: Landing page]*

> Here's the app. I enter my invite code...

*[Enter code, CTA appears, click through to /cards]*

> These are our persona decks — AI, Crypto, Macro, Famous Personalities. I'll open the AI deck. Each card is an inspectable contract — you can see exactly how the agent thinks: their biases, their epistemology, their flip conditions. Nothing is a black box.

*[Click one persona card briefly to show contract, then navigate to /setup]*

> Now I build my hand. I pick Karpathy, LeCun, Jim Fan, and Garry Tan. Topic: **"Will AI agents replace most knowledge workers within 10 years?"** I'll run it in Classical mode — deeper, turn-by-turn debate. Hit Deal.

## Live Demo — The Debate (1:15–2:05)

*[Screen: /match — debate streaming in real time]*

> First, Crux decomposes the topic into testable claims — not a vague question, but specific propositions like "Current LLMs can reliably automate 80% of analyst workflows."
>
> Each agent states their opening position — a stance and confidence level, grounded in things they've actually said publicly.

*[Debate turns start streaming]*

> Now they're debating. In Classical mode, the agent with the most urgency speaks next — they can challenge, concede a point, or introduce new evidence.

*[Point to sidebar]*

> Over here you can see convergence metrics updating live — entropy dropping as positions clarify, cruxes being surfaced in real time, and the agent polygon showing how stances cluster. LeCun and Karpathy are close on capabilities but diverge on timeline. Garry Tan is the outlier — he's pricing in disruption faster than the researchers.

## The Output (2:05–2:45)

*[Screen: Debate completes, click "View Full Analysis"]*

> Here's where the value is. This isn't a summary. It's a **disagreement map**.
>
> **Key Cruxes** — the pivotal questions driving the split. Example: "Will reasoning capabilities plateau before reaching general task automation?"
>
> **Fault Lines** — deep structural divides that facts alone can't resolve, like optimizing for different time horizons.
>
> **Flip Conditions** — LeCun changes his mind if autonomous agents pass a specific benchmark. Garry Tan changes his mind if enterprise adoption stalls for two consecutive quarters. Specific, testable, actionable.
>
> **Resolution Paths** — concrete ways the disagreement gets settled.

## Close (2:45–3:00)

> Three minutes ago you had a vague question about AI and jobs. Now you have a structured map of exactly where the smartest people disagree — and what it would take to settle it.
>
> That's Crux. The debate room for the internet's most influential voices.
