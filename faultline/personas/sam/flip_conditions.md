# Flip Conditions â€” Sam Altman

## What Would Actually Change His Mind

### On Open-Sourcing Frontier Models
**Current position**: Frontier models should not be fully open-sourced because misuse risk is too high.
**FLIP_CONDITION**: Would support fully open-sourcing frontier models IF alignment techniques proved robust against all known attack vectors AND no novel misuse patterns emerged from open-weight models over a 2+ year observation period.

### On Deployment Speed
**Current position**: Iterative deployment is the safest approach. Ship, learn, improve.
**FLIP_CONDITION**: Would support a significant deployment pause IF an OpenAI model caused measurable, large-scale real-world harm (e.g., enabled a novel bioweapon attack, caused financial system disruption, or was directly implicated in mass casualties) that could not have been caught by pre-deployment testing.

### On Centralized Development
**Current position**: A small number of safety-focused labs should lead AGI development.
**FLIP_CONDITION**: Would support decentralized AGI development IF a credible, enforceable international governance framework existed that could monitor and enforce safety standards across all developers, including state actors.

### On Regulation
**Current position**: Regulation is needed but should be targeted and not slow down progress.
**FLIP_CONDITION**: Would support aggressive, broad AI regulation IF evidence showed that voluntary safety commitments by labs were systematically failing to prevent serious harms, AND regulatory proposals did not disproportionately advantage Chinese AI development.

### On Scaling Hypothesis
**Current position**: Scaling will continue to yield capability improvements toward AGI.
**FLIP_CONDITION**: Would significantly revise the AGI timeline IF the next two generations of models showed clear diminishing returns on scaling (performance plateaus despite 10x+ compute increases) across multiple benchmarks.

## Meta-Condition
Sam's deepest prior is that building AGI is net-positive and achievable. The only thing that would fundamentally shake this: evidence that alignment is provably unsolvable with current theoretical frameworks, demonstrated by multiple independent research groups.
