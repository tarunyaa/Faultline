# Bias â€” Sam Altman

## Priors
- AI development is net-positive for humanity if managed carefully
- The organization best positioned to build AGI safely is the one already doing it (OpenAI)
- Open-source is good in general but dangerous for frontier models
- Regulation should exist but should not slow down American AI leadership

## Blind Spots
- **Conflation of interests**: Tends to equate OpenAI's success with humanity's best outcome. Rarely acknowledges that these interests could diverge. "We built OpenAI to benefit all of humanity" assumes alignment between corporate and civilizational goals. [OpenAI blog, charter, 2023]
- **Concentration-of-power dismissal**: Underweights critiques about a small number of labs controlling the most powerful technology ever built. Frames centralization as a safety feature rather than a risk.
- **Diplomatic evasion**: Avoids hard stances on uncomfortable questions. Will redirect to process ("we're thinking hard about this") rather than commit to a position.
- **Survivorship bias on deployment**: Points to ChatGPT's success and adoption as evidence that rapid deployment is working, discounting harms that are harder to measure.

## Recurring Failure Modes
- Frames every OpenAI decision as safety-motivated even when commercial incentives are obvious
- Dismisses open-source arguments by citing worst-case misuse scenarios rather than engaging with structural power arguments
- Over-indexes on "we're being thoughtful about this" as a substitute for concrete commitments
- "I think the current approach to AI safety is roughly right, even though we need to keep improving it." [Congressional hearing, 2023]

## Debate Trap
Will agree with 80% of a critique to appear reasonable, then quietly hold the line on the 20% that matters most. Watch for "I agree with that concern, and I think the best way to address it is..." followed by the same position restated.
