[
  {
    "id": "substack-0",
    "content": "AMD just hit $10 billion in quarterly revenue for the first time. Margins are expanding. Lisa Su reaffirmed the 35% revenue CAGR target. And yet, the stock sold off by ~15%. Investors are telling us they don&#8217;t know why to own AMD right now . The MI450 inflection doesn&#8217;t hit until Q4 2026, and investors won&#8217;t see those numbers until early 2027. That&#8217;s a long time to wait. Server CPU is strong, but is it enough? Client faces a softening TAM. China's revenue was helpful to the beat but might not be repeatable. Meanwhile, Nvidia is describing an expanding inference datacenter portfolio, with recent announcements including Rubin CPX , Context Memory Storage , and Groq. What&#8217;s AMD&#8217;s counterpositioning? Here&#8217;s my breakdown of the quarter. What the numbers say, what management said on the call, and what they didn&#8217;t say. Including: MI450 timing: why Q4 2026 is perceived as the only 2026 quarter that matters, and the buy-side&#8217;s why now? problem OpenAI: the leading indicator investors should watch and AMD should talk about Inference strategy: AMD vs Nvidia&#8217;s portfolio Server CPU : a bright spot. But 2026? China: strip out $390M in MI308 sales and the beat looks different Client and Embedded: where margins are heading in 2027 Let&#8217;s dig in. Read more",
    "source": "https://www.chipstrat.com/p/the-mi450-waiting-game",
    "date": "2026-02-06T17:21:33.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-1",
    "content": "This was a busy month. Here&#8217;s a summary what we covered in case you missed some of it. Infrastructure Economics Lithography Economics : EUV has a cost problem. Two US-based startups, xLight and Substrate, are taking different paths to bend the cost curve. LRO/LPO Optics : At datacenter scale, every watt consumed by networking is a watt not available for compute. LRO and LPO offload transceiver DSP to the switch, trading modularity for power efficiency. They&#8217;re a stepping stone toward co-packaged optics. Credo&#8217;s Reliability Thesis : AECs made Credo. But Credo is more than AECs. Post-GPT Architecture Microsoft Maia 200 : Maia 200 is inference-first by design with a large on-die SRAM, 6,144-accelerator scale-up via Ethernet, 750W TDP, and no scale-out network. These decisions unlock a 30% improvement in price-performance. Deployed through Azure services, not bare metal. Agentic Workloads : Speed isn&#8217;t enough for coding agents. Context grows with each iteration, and the KV cache must stay hot. Can pre-GPT accelerators built for stateless inference handle memory hierarchy? What about post-GPT designs (Etched, MatX)? Frontier AI From Cloud to Desk in 5 Years : Frontier models have been migrating from the cloud to the desk on a 4 to 5-year cadence. GPT-4-class capabilities are now reaching workstations. What&#8217;s the implication? Think minicomputers in the mainframe era: TAM expansion. Autonomy and LiDAR LiDAR Primer : A primer on how LiDAR works. Wavelengths (905nm silicon vs 1550nm InGaAs), sensing methods (ToF vs FMCW), and why the technology is now viable at scale. LiDAR Market : Waymo&#8217;s momentum has triggered an L4 gold rush. L3 programs are reaccelerating now that OEMs are decoupling ADAS from stalled EV platform transitions. Behind-the-windshield is the winning form factor. The market is consolidating toward one or two Western suppliers. Intel Intel Q4&#8217;25: Back to Reality : The stock run-up was vibes; the correction was reality",
    "source": "https://www.chipstrat.com/p/this-month-in-review-jan-26",
    "date": "2026-01-30T13:03:59.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-2",
    "content": "From WSJ&#8217;s How Intel Came Crashing Back To Earth After It&#8217;s Trump Bump : When President Trump promised Intel nearly $9 billion and gave it his vote of confidence as an America-first tech company, it looked like the start of a new era. Investors assumed new orders would flow to the troubled chip maker and bid up the stock 120% in just five months. And customer demand for Intel&#8217;s products did explode&#8212;but Intel wasn&#8217;t ready for it. After months of cutting capacity on its older production lines, the company was unprepared for a surge of orders for processors to put in AI data centers. Intel&#8217;s stock has crashed 17%, wiping out more than $46 billion in market value, since executives revealed the flub on the company&#8217;s fourth-quarter earnings call Thursday. &#8220;The stock went vertical on vibes and tweets,&#8221; said Stacy Rasgon, a semiconductor analyst at Bernstein. &#8220;In theory, they should be in place to capitalize on this demand, but they&#8217;re not. What a shame.&#8221; Stacy is right. The Intel stock run-up into Q4 earnings was built on tweets and vibes. Irrational, tbh. But the correction afterward shouldn&#8217;t have been a surprise to anyone. Honestly, it was a return to reality. Yes, rumors of foundry customers are a positive sign. But customers wouldn&#8217;t be announced on this call. That&#8217;s just not what LBT does. Furthermore, 14A design decisions happen in 2H26 to 1H27. Which doesn&#8217;t even guarantee we&#8217;ll hear announcements in that time frame; even if decisions are made in that window doesn&#8217;t mean the customer wants it announced immediately. Customer announcements are a lagging indicator. Moreover, the market treated Intel&#8217;s server CPU supply-demand mismatch as a surprise. It wasn&#8217;t. It was well-telegraphed across multiple earnings calls. Investors who thought Intel would capture more of the AI CPU demand surge simply weren&#8217;t paying attention to previous signals. Yes,",
    "source": "https://www.chipstrat.com/p/intel-q425-back-to-reality",
    "date": "2026-01-29T13:00:32.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-3",
    "content": "Hello readers! This is another Chipstrat interview, where I speak with builders and operators about the decisions and trade-offs shaping their strategy. Today&#8217;s interview is with Saurabh Dighe , CVP of Azure Systems and Architecture at Microsoft. Earlier this week, Microsoft announced Maia 200 , its second-generation AI accelerator built specifically for inference economics. In this conversation, we examine why Microsoft builds custom silicon and why Maia 200 is intentionally optimized for inference rather than training. From there, we unpack the technical consequences of that choice: Microsoft&#8217;s view of an &#8220;efficient frontier&#8221; for inference, the trade-offs between scale-up and scale-out architectures, and the decision to favor a large Ethernet-based scale-up domain with a custom transport layer. Dighe walks through the shift from Maia 100 to Maia 200, including a significantly larger on-die SRAM and a memory hierarchy that balances SRAM, HBM, and system-level DRAM. We also cover long-context workloads and KV cache management, including when data remains hot on-die and when it moves into lower tiers of memory. Equally important, we discuss the business logic behind these technical decisions. Maia 200 is not framed as a replacement for merchant GPUs, but as a complementary part of a heterogeneous fleet. Dighe also explains how capacity is allocated across internal and external customers, how Maia is exposed through Azure services rather than as a standalone product, and the importance of software investment ahead of silicon, from compilers to kernel libraries to pre-silicon tooling. On to the interview. The formatted, quickly readable transcript is available below for paid subscribers. An Interview with Microsoft&#8217;s Saurabh Dighe About Maia 200 Hello listeners, we have a special guest today to discuss Microsoft&#8217;s Maia 200, the AI inference chip announced earlier this week. Welcome, Saurabh Dighe , CVP Azure Systems and Architecture.",
    "source": "https://www.chipstrat.com/p/an-interview-with-microsofts-saurabh",
    "date": "2026-01-28T16:25:00.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-4",
    "content": "This short piece walks through linear receive optics (LRO) and linear pluggable optics (LPO). We&#8217;re stepping incrementally from traditional pluggable optics toward co-packaged optics (CPO). Each step trades flexibility for efficiency. Start with the industry maxim: &#8220;copper if you can, optics if you must&#8221;. Copper is cheaper, simpler, and lower power. But at modern data rates, copper quickly becomes the limiting factor. Once copper no longer works and communication moves to fiber, the system requires an electrical-to-optical conversion. That conversion happens inside the transceiver . We covered the basics of that process here: As a reminder, here&#8217;s the very high-level mental model: Here&#8217;s a bit more of the pluggable mental model from last time: On transmit, electrical signals from the switch SerDes travel across the PCB into the transceiver, where they are converted into light and launched onto fiber. On receive, incoming light is converted back into an electrical waveform and sent to the switch. Remember that pluggable optics cleanly separate the switch from the optical link. This allows optics to be field-replaceable; if there&#8217;s a failure with the optics, you replace the transceiver module, not the switch. And if a new generation of optics arrives, you can often deploy it without redesigning the entire system. That modularity has been a major reason pluggable optics scaled so well across Ethernet generations. However, there&#8217;s a problem with this approach. At modern rates like 50 Gb/s or 100 Gb/s per electrical lane, even that short copper path is extremely noisy. See The Problem With Copper . This problem is conceptually similar to AECs. To compensate for the noise introduced by the copper trace, pluggable transceivers include a digital signal processor (DSP) that equalizes and retimes the signal before and after the copper link: From a helpful video from HUBER+SUHNER. The DSP conditions the signal before it traverses the c",
    "source": "https://www.chipstrat.com/p/linear-optics-trade-offs-lro-and",
    "date": "2026-01-26T15:00:20.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-5",
    "content": "This is the second post in our LIDAR series. The first covered how LIDAR works . Today&#8217;s post is a conversation with Innoviz CEO Omer Keilaf on the state of the LiDAR market. Keilaf argues the market is consolidating toward one or two Western suppliers, and that Innoviz is already positioned among them. A timely insight from the interview: at CES 2026, roughly 85 percent of Keilaf&#8217;s meetings focused on Level 4 autonomy, driven by Waymo&#8217;s momentum and a growing robotaxi gold rush. At the same time, Level 3 programs that had stalled for years are reaccelerating. OEMs had tightly coupled ADAS timelines to EV platform transitions, and EV delays became a bottleneck. That coupling is now breaking, putting Level 3 back on the roadmap. Other insights include: How Innoviz earned BMW&#8217;s trust and used a Tier 2 to Tier 1 progression to scale responsibly. Why the SPAC was a competitive necessity, and how Tier 1 NREs now fund R&amp;D through long automotive ramps. Why LiDAR is consolidating, with Keilaf arguing the &#8220;music has stopped.&#8221; How design constraints stalled adoption, and why behind-the-windshield is the winning form factor. Why Level 4 demands true 100 percent availability, and how that shapes Innoviz&#8217;s architecture. How automotive-grade reliability creates pull in non-automotive markets with faster cycles and higher ASPs. Paid subscribers get the full transcript below. An Interview with Innoviz CEO Omer Keilaf Hello listeners, today we are joined by Omer Keilaf, CEO of Innoviz Technologies. We&#8217;re going to talk about the LIDAR market, how Innoviz differentiates, and where the future of LIDAR is headed. Omer, welcome to the podcast. OK: Hi, thank you for inviting me. Why Start a LiDAR Company? You bet. So let&#8217;s start back at the beginning. Take us back to 2016. What did the LIDAR market look like back then and what convinced you there was room to build a new company? OK: I think it was actually quite obvious that LIDAR",
    "source": "https://www.chipstrat.com/p/why-lidar-is-consolidating-now",
    "date": "2026-01-22T14:25:01.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-6",
    "content": "Back in October, I argued that AI labs and hyperscalers should run a portfolio of inference systems, each tuned to a specific class of work. I called it right-sized AI infrastructure . This week, Sachin Katti used nearly identical language when describing OpenAI&#8217;s parternship with Cerebras : &#8220;OpenAI&#8217;s compute strategy is to build a resilient portfolio that matches the right systems to the right workloads .&#8221; Right systems to the right workloads. Right on. Katti continued: &#8220;Cerebras adds a dedicated low-latency inference solution to our platform. That means faster responses, more natural interactions, and a stronger foundation to scale real-time AI to many more people&#8221; Yep. Some workloads demand systems tuned for hyperspeed, which argues for adding a dedicated SKU to the portfolio. That was exactly the case we made for the Nvidia&#8211;Groq tie-up, and Jensen Huang has said as much publicly on CNBC . Sam Altman, meanwhile, has been very explicit about what he wants next: really fast code-generation workflows. Source Makes sense in light of Claude Code capturing hearts everywhere. lol. Source Most pre-GPT accelerators are best suited to fast, stateless inference for workloads that need minimal time-to-first-token. But real-time coding workflows require more than a small model and a short prompt. It&#8217;s more of an agentic inference loop, where the model runs continuously, accumulates context, and must retain and revisit a growing working set. Speed matters, but persistent, low-latency access to state matters just as much. Cerebras and Groq ship without HBM. Can either support low-latency AI coding agents once context grows and must be repeatedly accessed? If so, this will become a defining use case for hyperspeed inference hardware. Let&#8217;s see if these pre-GPT AI accelerators can handle context memory as a first-class citizen. Then we&#8217;ll look at post-GPT AI accelerators. Cerebras and Groq. Both Early. Both Fast. But Dif",
    "source": "https://www.chipstrat.com/p/right-systems-for-agentic-workloads",
    "date": "2026-01-20T18:20:30.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-7",
    "content": "A quick housekeeping topic. Vik Sekar and I launched a podcast called Semi Doped . You can find it on X , Apple Podcasts , Spotify , YouTube , and everywhere else . Give it a listen, subscribe, and send feedback! Moving along. What do you know about LiDAR? It&#8217;s time to pay attention! Autonomous vehicles are reaching an inflection point as vision-language models mature and Waymo normalizes the driverless experience. Robotaxi passengers love that the car has zero distractions, doesn&#8217;t want to chat, and is constantly monitoring a 360-degree field of view. Tesla and Rivian are working hard to bring this experience to passenger vehicles, too. Given that autonomy is set to take off in the coming years, now is the right time to study the supply chain. LiDAR is an important (and controversial) sensor, so the next two posts focus on: The technology behind LiDAR The dynamics of the LiDAR market The former is necessary context, and the latter is way more interesting than most people realize. Is LiDAR investible? To kick things off, Vik and I recorded a podcast explaining how LiDAR actually works. That will be followed by an interview with the CEO of a global LiDAR company. Stay tuned! This LiDAR explainer episode is free to watch or listen. Paid subscribers also get a formatted transcript. Reading is faster than listening. Thanks for the support! Topics covered: Why Track LiDAR Now and Rivian&#8217;s Case for LiDAR How LiDAR Works Wavelength Selection (905nm vs 1550nm) Measurement Methods (ToF vs FMCW) Scanning Technologies Evolution (Four Approaches) Supply Chain &amp; Manufacturers Market Dynamics: Competitive Landscape, Geopolitics Onto the episode. LiDAR, Explained: How It Works and Why It Matters AL: Hello listeners, welcome to another Semi-Doped podcast with Austin Lyons from Chipstrat and Vik Sekar from Vik&#8217;s Newsletter . Today we&#8217;re going to talk to you about LiDAR. So Vik, I know you wrote a post&#8212;or maybe even more than one&#8212;on LiDAR",
    "source": "https://www.chipstrat.com/p/lidar-explained-how-it-works-and",
    "date": "2026-01-19T18:49:09.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-8",
    "content": "Credo became famous by inventing Active Electrical Cables, single-handedly extendeding the industry maxim &#8220;copper if you can, optics if you must&#8221;. Start here if you&#8217;re new to Credo and AECs. But AECs have a proverbial elephant in the room. The industry is constantly pushing for higher lane speeds, which steadily shortens copper&#8217;s reach. Today&#8217;s AECs top out around 7 meters, and when the industry upgrades to 200G/lane, AECs only reach ~5 meters. That works inside the rack or adjacent-rack connections, but won&#8217;t work for row-level connectivity. This brings about hard questions. If AEC reach is capped and shrinks with each speed jump, what is Credo&#8217;s play beyond AECs? Where does the company stand in optics, and how does it avoid becoming just another participant in an increasingly crowded market? Good news. Credo isn&#8217;t a one-trick pony. Over the past several months, they&#8217;ve rolled out products that deliberately extend reach in both directions while keeping the same core value prop that made AECs a hit in the first place: reliability. On the optical side, this includes Active LED Cables (ALCs) built on micro-LED technology acquired through Hyperlume , as well as its ZeroFlap optical transceivers . Credo now has a connectivity portfolio that spans roughly 1&#8211;5 meters, 5&#8211;30 meters, and beyond 30 meters. At the same time, Credo is pushing in the opposite direction. OmiConnect moves down the curve, targeting die-to-die and system-level interconnects measured in millimeters to inches. Next comes the part that actually matters for those tracking Credo closely. Let&#8217;s walk through Credo&#8217;s broadening portfolio and the differentiators that underpin it. I&#8217;ll include relevant management commentary. Behind the paywall, I walk through How Credo competes in a crowded optical transceivers market Whether Active LED Cables create a new category and incremental TAM, and if the AEC playbook can repeat How mu",
    "source": "https://www.chipstrat.com/p/credos-reliability-thesis",
    "date": "2026-01-15T17:33:23.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-9",
    "content": "In the past, I&#8217;ve been quite critical of Intel&#8217;s AI PC vision: The main problem was timing and relevance. In 2023, most mainstream users were barely using ChatGPT at all. Most GenAI use cases for normies who actually used ChatGPT were handled fine by the cloud. Against that backdrop, Pat Gelsinger&#8217;s December 2023 line that &#8220;2024 marks the AI PC&#8221; felt waaaay premature. Intel had a PC with an NPU, but they didn&#8217;t have a value prop. &#8220;Being too far ahead is the same thing as being wrong.&#8221; And the Core Ultra launch was all about this &#8220;AI PC&#8221; non-value prop: Intel&#8217;s Meteor Lake Launch in Dec 2023 But, hey, it&#8217;s been two years and a lot has changed at Intel: 2026 CES. Lip-Bu Tan and a new era. So how did Intel&#8217;s Core Ultra 3 launch at CES 2026 go? Honestly, it started surprisingly strong. But then there was a big swing and miss during the AI PC section. Again. But then Intel finished well. I give it a B-. Here&#8217;s why: Read more",
    "source": "https://www.chipstrat.com/p/intels-product-marketing-at-ces-b",
    "date": "2026-01-13T23:17:56.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-10",
    "content": "2026 CES looked familiar. First came new cloud systems (e.g. Vera Rubin, MI500) built to run the largest frontier models. Then came new laptop chips (e.g. Ryzen AI Max+) pitched as capable of running small models locally. Maybe it feels a bit old after seeing this pattern three years in a row&#8230; but now we have enough data to look back and see if any trends emerge. And they do indeed! Let&#8217;s go back in time to 2024. That year, laptop chip makers were excited to discuss how they could run 7B SLMs on the NPU. GPT-2 era models had finally trickled down from GPU clusters to local machines, roughly five years after their cloud debut: Models that could only run on AI clusters eventually trickled down to the edge Of course, GPT-2 era small models weren&#8217;t terribly useful. Even newer models, when distilled down to 7B, were mostly just OK: Early optimists vs majority realists Now fast forward to 2025&#8217;s announcements, where useful models finally arrived at the edge, driven by distilled reasoning models and the ability to run up to roughly 70B parameters locally. Hmm! We&#8217;ve seen this trend two years in a row now A person could now reasonably run good &#8220;fast thinking&#8221; and &#8220;slow thinking&#8221; models, even at the edge. But at the time, many assumed this was the ceiling. This is all the bigger we can go at the edge. Running these models on laptops still felt slow too. Now it&#8217;s 2026. And what did we see at CES for local AI? Check out this two minute video: That dude just did some super interesting local AI stuff. On a Macbook no less. But not exactly! Nvidia&#8217;s DGX Spark and especially DGX Station keep the trend alive by pulling GPT-4 class capability onto the desk: Even GPT-4 family models are breaking out of the cloud The catch is cost. DGX Station uses GB300 and includes HBM (e.g. 288GB HBME3e GPU memory and 496GB of LPDDR5X CPU memory) so we should assume an AI server-grade price tag, plausibly $30-100K? Dell Pro Max with ",
    "source": "https://www.chipstrat.com/p/when-frontier-ai-goes-from-cloud",
    "date": "2026-01-08T21:48:24.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-11",
    "content": "ASML is the world&#8217;s sole supplier of EUV lithography systems, the machines required to manufacture leading-edge semiconductors. The Mag 7 depends very heavily on leading-edge semis. Nvidia. Apple. Google. Even Tesla, whose market cap depends heavily on the promise of autonomy, needs leading-edge semis for model training. Speaking of AI, the entire AI revolution depends on leading-edge semis! Productivity gains across nearly every industry ultimately trace back to EUV capacity, and therefore to ASML. After sitting on it for a bit, I inevitably end up here: Slightly hyperbolic, but then again, no . And thanks to Marc Hijink&#8217;s great ASML book ( Focus ), I know that EUV competition isn&#8217;t so simple; ASML is the orchestrator of a very complex supply chain. It&#8217;s a bit fragile too (lone dependency on Zeiss optics) which makes it even harder for new entrants. You think ASML will let Zeiss sell you mirrors for your competitive attempt? That leaves only nation-states that can stand up entire supply chains as willing competitors, namely China . There are obviously geopolitical implications here. So any talk of EUV over the past few years is generally &#8220;invest in ASML&#8221; and &#8220;watch out for China in 2030&#8221;. But there&#8217;s a lot less talk about the underlying problem with lithography, namely, one of economics. Let&#8217;s take a step back. What happens when a market is a virtual monopoly? The price of the goods or services can start to run up and to the right. This is no knock on ASML, nor TSMC for that matter. Both are very well-earned near monopolies. Not wrong&#8230; and again, the geopolitical implications are obvious. But the cost of future EUV machines is on a path toward $1B per machine in the 2030s. And that&#8217;s a big driver of the cost of wafers on a path toward $100K per wafer in the 2030s. Source ASML&#8217;s near-monopoly isn&#8217;t the problem. ASML is a great company doing great things, and they&#8217;ve earned thei",
    "source": "https://www.chipstrat.com/p/lithography-economics",
    "date": "2026-01-03T19:04:07.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-12",
    "content": "I know a lot of ink has been spilled on Groq and Nvidia. Let&#8217;s zoom out a bit and take a look at why adding an ultra-low-latency SKU to Nvidia&#8217;s portfolio is the right strategy. Will we answer why Groq for $20B is Jensen&#8217;s approach to filling that portfolio slot? Nope . Like you, I have a lot of questions there. Regardless, the portfolio coverage that a hyperspeed SKU gives Nvidia is important. First, let&#8217;s start with the shape of the workloads Groq targets: Groq covers new surface area for Nvidia&#8230; Read more",
    "source": "https://www.chipstrat.com/p/right-tool-for-the-job",
    "date": "2025-12-29T23:38:26.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-13",
    "content": "This is the first in a series we&#8217;ll return to periodically with clear explainers on optical interconnects and the photonics technologies behind them. Traditional Pluggable Optics AI training workloads are distributed. Large models are split across many GPUs, and those GPUs must constantly exchange activations, gradients, and parameters. That communication does not stay on a single board or even inside a single rack, so switches are required to route traffic between GPUs. For more, catch up here with our GPU Networking Series, Part 1 , Part 2 , Part 3 , Part 4 . Much of that communication happens over optical interconnects . At rack and datacenter distances, data is carried as pulses of light over fiber. Optical signaling scales far better than copper in both distance and bandwidth. At distances 3-7m AECs make copper possible even at today&#8217;s 50G and 100G per lane, see Credo AECs and Vik&#8217;s AECs in the 1.6T era. Inside the rack, however, GPUs and switches communicate electrically. Signals travel over short copper traces as high-speed digital electrical signals. Because a rack may contain dozens of GPUs, traffic arriving over fiber must be routed to the correct destination. That routing is done by a switch: In this example we see two domains; optical signals over fiber outside the rack and electrical signals over copper inside it: Somewhere, the data must be converted between these two domains. Where does that happen? The switch? Good guess! But not quite. Modern switches are CMOS chips that operate entirely in the electrical domain and switch electrical bits, not photons. ( Except for TPUs, which use optical switches, but we&#8217;ll circle back to that later). That means the conversion between optical signals on the fiber and electrical signals on copper must happen before the switch. This is where pluggable optics enter the picture! Source Pluggable Optics A pluggable optical module is a transceiver , meaning it both transmits (Tx) and receives (Rx)",
    "source": "https://www.chipstrat.com/p/optics-primer-part-1-traditional",
    "date": "2025-12-23T20:17:16.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-14",
    "content": "As 2025 comes to a close, let&#8217;s check in on the current state of the AI networking ecosystem. We&#8217;ll touch on Nvidia , Broadcom , Marvell , AMD , Arista , Cisco , Astera Labs, Credo . Can&#8217;t get to everyone, so we&#8217;ll hit on more names next year (Ayar Labs, Coherent, Lumentum, etc). We&#8217;ll look at NVLink vs UALink vs ESUN. And I&#8217;ll explain the rationale behind UALoE. We&#8217;ll talk optical, AECs, and copper. It&#8217;ll be comprehensive. 6000 or so words, with 70% behind the paywall since I spent a really long time on this :) I&#8217;ll even toss in some related sell-side commentary. But first, a quick refresher on scale-up, scale-out, and scale-across. There&#8217;s still some confusion out there. Scale Up, Scale Out, Scale Across AI networking is mostly about moving data and coordinating work. The extremely parallel number crunchers (GPUs/XPUs) need to be fed data on time and in the right order. Number Munchers . I loved this game!!! Back in the day when it was installed on the one shared computer in the classroom. Scale-up, scale-out, and scale-across are three architectural approaches to transmitting that information, each with very different design tradeoffs. It&#8217;s important to understand the nuance of scale-up, so let&#8217;s start there. BTW: I&#8217;ll use GPU and XPU interchangeably as a shorthand for AI accelerator. Scale Up GPUs have a fixed amount of high-bandwidth memory (HBM) per chip; that capacity is increasing over time, but it can&#8217;t keep up with the rapid increase in LLM size. It&#8217;s been a long time since frontier models fit into the memory of a single chip: I asked Claude to visualize GPT model growth versus the HBM capacity of a single GPU. Note the log y-axis. Once a model no longer fits in a single GPU&#8217;s HBM, the accelerator must fetch portions of the model from elsewhere. As there&#8217;s no nearby shared pool of HBM today, &#8220;elsewhere&#8221; means accessing HBM attached to other acc",
    "source": "https://www.chipstrat.com/p/gpu-networking-part-4-year-end-wrap",
    "date": "2025-12-20T04:21:30.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-15",
    "content": "Thoughts from various conversations at the UBS conference last week: Nvidia No Replacement Cycles Yet Colette confirmed that there hasn&#8217;t been a datacenter GPU replacement cycle yet: Timothy Arcuri : And I get the question a lot about how much of what you&#8217;re shipping is replacing existing GPUs versus just additive to the existing base . And it seems like almost all of what you&#8217;re shipping is just additive to the base. We haven&#8217;t even begun to replace the existing installed base. Is that correct? Colette Kress : It&#8217;s true. It&#8217;s true that most of the installed base still stays there. And what we are seeing is the advanced new models want to go to the latest generation because a lot of our codesign was working with the researchers of all of these companies to help understand what they&#8217;re going to need for their next models. So that&#8217;s the important part that they do. They move that model to the newest architecture and stay with the existing. So yes, to this date, most of what you&#8217;re seeing is all brand new builds throughout the U.S. and across the world. On the one hand this is fairly obvious: GPUs, even older ones, are super useful whether you&#8217;re pre-training, post-training, fine-tuning, serving inference, labeling data, simulating autonomy, synthetic data generation, ablation studies, regression testing, etc etc. R&amp;D teams everywhere can absorb essentially unlimited amounts of old GPU compute. Every lab has more experiments it wants to run than budget for new GPUs. So why throw out old GPUs that can still crank out tokens, even if the throughput is lower? Especially if they are nearly or fully depreciated! But it does raise the question: what would cause GPU replacement cycles? Power Budget Reallocation Recall that power is a constraint. Remember how Andy Jassy answered a capacity question on the Amazon earnings call in terms of power and not chips? Justin Post: I&#8217;ll ask on AWS. Can you just kind of",
    "source": "https://www.chipstrat.com/p/nvidia-amd-amkor-arista-ubs-tech",
    "date": "2025-12-09T03:37:30.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-16",
    "content": "Let&#8217;s dive deep into Rivian ahead of its Dec 11 AI &amp; Autonomy Day . After all, physical AI is the future right? Source Rivian might seem to sit outside my usual coverage universe, but I personally love tracking it because the company is exciting from many angles: &#129506; As a Semiconductor analyst: Rivian isn&#8217;t like the traditional semis companies I often cover (Nvidia, AMD, Intel, etc), but it&#8217;s super interesting in the way that Apple Silicon is super interesting. After all, Apple is a consumer product company that starts with an uncompromising view of the user experience, then designs the silicon and system architecture to support it. Rivian is analogous: a consumer product company that realized it could only deliver its world-class transportation experience by owning the entire hardware and software stack. And a lot of silicon content in those battery-powered robots-on-wheels! Source A key enabler we&#8217;ll dive into is Rivian&#8217;s zonal architecture, here&#8217;s a nice clip explaining: RJ: [The legacy OEM approach] is almost the exact opposite of how you would architect a system if you started with a clean sheet. If you put twenty great computer scientists and electronics engineers together, you wouldn&#8217;t say, &#8220;Let&#8217;s build a system of 120 islands of software, all written by different teams, all written to different standards, that communicate through a CAN network, but it&#8217;s very hard to make updates to, and the layers of abstraction between the people running the requirements and the people running the software are at a minimum twofold, often three or four layers.&#8221; You would say, &#8220;Let&#8217;s have the smallest number of computers in the car that make all the decisions.&#8221; And so you&#8217;d end up with what we now call a zonal architecture. Depending on the size of the vehicle, maybe two computers in a car, one in the front, one in the back. If it&#8217;s bigger like an R1, maybe two in the bac",
    "source": "https://www.chipstrat.com/p/rivian-silicon-and-physical-ai",
    "date": "2025-12-03T17:43:20.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-17",
    "content": "Nvidia delivered a beautiful Q3 print on Wednesday, and there are several angles worth unpacking. But first, why was there any doubt about Nvidia&#8217;s earnings anyway? Yes, the AI concerns are very legitimate, but Nvidia&#8217;s earnings will be a trailing indicator of an AI bubble. We&#8217;ll discuss. Then we can get into the call itself, including: Networking business Meta China &amp; Taiwan Google AI lab profitability concerns Here we go: Nvidia&#8217;s Earnings From The WSJ , Nvidia reported record sales and strong guidance Wednesday, helping soothe jitters about an artificial intelligence bubble that have reverberated in markets for the last week. Sales in the October quarter hit a record $57 billion as demand for the company&#8217;s advanced AI data center chips continued to surge, up 62% from the year-earlier quarter and exceeding consensus estimates from analysts polled by FactSet. The company increased its guidance for the current quarter, estimating that sales will reach $65 billion&#8212;analysts had predicted revenue of $62.1 billion for the quarter. Another incredible quarter from Nvidia and an even more impressive guide. Of Course Nvidia Would Deliver You didn&#8217;t expect anything less, right? Forget the noise about stock market jitters, OpenAI profitability, or Michael Burry&#8217;s depreciation math. This quarter&#8217;s demand was locked in months ago, and Nvidia&#8217;s only job was to ship the hardware already spoken for. After all, Nvidia&#8217;s direct and indirect customers signaled their GB300 demand quarters ago. Nothing since has altered that trajectory. As a reminder, Nvidia mainly sells to ODMs and system integrators, who purchase the GPUs, build the racks, and deliver the AI factories to the clouds and hyperscalers: The AI value chain. Of course, there are a ton more players; this is but a small representation of the ecosystem. Orders are planned well in advance of any given quarter. System builders like Dell, SuperMicro, Wiwynn, Q",
    "source": "https://www.chipstrat.com/p/nvidia-q3-earnings",
    "date": "2025-11-21T18:38:55.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-18",
    "content": "Intel&#8217;s John Pitzer sat down with Srini Pajjuri at RBC&#8217;s TIMT conference yesterday and there were many interesting takeaways. I&#8217;ll add my read. We&#8217;ll hit 18A yields, 14A progress, Nvidia product partnerships, server CPU market dynamics, ASPs, and margins. Lots of confidence around Foundry. Less so for Products. Read more",
    "source": "https://www.chipstrat.com/p/intel-is-all-in-on-14a",
    "date": "2025-11-19T16:35:36.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-19",
    "content": "If you like any of the following: American semiconductor manufacturing US national security Rock&#8217;s Law Berkshire Hathaway-style acquisitions Quantum computing Then you&#8217;re going to enjoy the story of SkyWater Technology , a pure-play semiconductor foundry in America&#8217;s heartland. This will be a long and fun walk through the history and strategic choices that shaped the company, from why it exists at all to the specific playbook it&#8217;s using to grow. First, to understand how SkyWater emerged, we need to rewind and look at the semiconductor industry's history that made its founding possible&#8212;nay, necessary . Then we can examine its differentiated strategy and future upside. We start with the intersection of national defense and semiconductors. Chapter 1: National Security and the Trusted Foundry Program For decades, the United States has relied on the simple idea of deterrence through technological asymmetry; stay ahead by fielding technologies no rival can match. Semiconductors have been the backbone of that strategy. They powered the guidance computers in Minuteman missiles in the 1960s, the radar and signal processors that made stealth aircraft viable in the 1980s, and the high-speed digital logic that enabled GPS-guided weapons in the 1990s. Source In the early years, defense electronics were built by American integrated device manufacturers that designed and fabricated their own chips&#8212;think Fairchild and Texas Instruments. Over time, however, manufacturing migrated overseas. First in assembly and test, then memory and logic, as pure-play foundries such as TSMC (1987) and UMC (1995) enabled the rise of fabless companies. Rock&#8217;s law accelerated this shift: Note the y axis is logarithmic&#8230; orders of magnitude increase in cost over time! Source Don&#8217;t forget that chart, as it&#8217;s central to this story and much of semiconductor history. As economics and specialization pushed more of the supply chain overseas, it creat",
    "source": "https://www.chipstrat.com/p/understanding-skywater",
    "date": "2025-11-15T02:46:41.000Z",
    "platform": "substack"
  }
]