[
  {
    "id": "substack-0",
    "content": "For a long time, NAND flash memory has been regarded as the &#8220;second-class citizen&#8221; of the semiconductor industry&#8212;a storage component with strong commodity attributes subject to cyclical fluctuations. However, at the beginning of 2026, we are witnessing a profound &#8220;handover of power&#8221;. SanDisk&#8217;s financial reports this week confirm that the demand logic, supply structure, and even the valuation system of the NAND industry are undergoing an unprecedented reconstruction. The Four Underlying Logics of NAND Demand Storage as Computation: Using storage to trade for computing power&#8212;specifically the persistence of KV Cache &#8212;can significantly reduce power consumption during the prefill stage. SanDisk&#8217;s CEO explicitly mentioned in an earnings call that token intensity is accelerating and storage has become a key enabler for AI inference. They preliminary estimate that KV cache will bring an additional 75-100 EB of demand in 2027, doubling a year later. This directly confirms the scale and urgency of &#8220;storage as computation&#8221;. Shift in Data Generation Subjects: Data production is shifting from human-generated content to self-generation by models within symbolic space, unrestricted by time, attention, or physical boundaries. SanDisk noted that &#8220;data growth is accelerating as the temperature of data is rising,&#8221; which corresponds to the logic that more data deserves persistent storage as the generation subject changes. Increased Value of Data Reuse: In the past, the global storage rate was only 1%; now, LLM/RAG (Retrieval-Augmented Generation) has awakened historical data, significantly increasing storage rates. SanDisk emphasized increased content during the inference stage, the accelerated adoption of Enterprise SSDs, and that data centers will become the largest NAND market in 2026 (with growth in the high 60s Exabytes), all reflecting the compound pull of skyrocketing reuse value. Data Inflation Under ",
    "source": "https://globalsemiresearch.substack.com/p/the-handover-of-power-in-nand",
    "date": "2026-01-30T15:27:55.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-1",
    "content": "Micron announced plans Tuesday to build a $24 billion NAND facility in Singapore. Our take is straightforward: this reinforces our thesis that NAND demand will persist and strengthen. A long-term investment of this magnitude signals that Micron is positioning for the next decade, not the next quarter. The company clearly sees structural growth ahead. Micron&#8217;s Singapore investment is a signal, not just a capacity announcement. It reflects the company&#8217;s conviction in AI-driven storage demand and its intention to capture a strategic position in next-generation technologies like High-Bandwidth Flash (HBF). For investors watching the memory sector, this is validation that the AI infrastructure buildout extends well beyond GPUs and high-bandwidth memory&#8212;storage is the next frontier. What Micron Plans to Build Beyond the headline investment figure, we&#8217;re particularly interested in the technologies Micron plans to develop at this facility. According to our industry checks, Micron is planning to develop High-Bandwidth Flash (HBF) technology at the new Singapore facility among other advanced NAND capabilities. We believe the HBF component is directly tied to Nvidia&#8217;s roadmap and represents a strategic shift in AI infrastructure. Our checks show that Nvidia is planning to deploy HBF technology as part of its Integrated Compute Memory Storage (ICMS) architecture. The driver is simple: enterprise SSD bandwidth is insufficient for next-generation AI compute requirements. As AI models scale and inference workloads multiply, storage I/O is becoming the bottleneck. We&#8217;ve also learned that Nvidia is currently partnering with SanDisk on HBF development, suggesting the company is securing multiple supply sources for this critical technology. Micron&#8217;s $24 billion commitment appears to position itself as a key supplier in this emerging category. If our read is correct, this isn&#8217;t just a NAND fab&#8212;it&#8217;s infrastructure for the next ",
    "source": "https://globalsemiresearch.substack.com/p/reading-microns-24b-singapore-move",
    "date": "2026-01-27T10:20:22.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-2",
    "content": "Recently, CPU prices have been consistently rising. According to information from CPU distributors, the price of consumer-side CPUs has increased by approximately 30% since last August, and server-side CPUs have also recently begun to see price hikes. In the capital markets, Intel&#8217;s stock price has risen by more than 30% this year, and several Chinese stocks related to CPU concepts have performed very well. This trend has even expanded to PCIe IP companies, as the market believes that increased CPU demand will benefit PCIe IP providers. This article examines the primary reasons for the recent CPU price increases and whether this is a short-term phenomenon or a trend that will persist. Reasons for CPU Price Increases First is the supply side: TSMC&#8217;s production capacity is strained. TSMC&#8217;s advanced processes (such as N2) prioritize AI chips, while CPUs primarily use sub-high-end processes (N3E), leading to restricted capacity allocation. Second, the supply of general semiconductor equipment (such as testing equipment) and consumables (such as photoresist and BT substrates) required for production is tight, indirectly affecting the capacity release of CPU manufacturers. Consumer-side CPUs benefit from flexible price adjustments (quarterly or bi-quarterly), and the impact on terminal customers is relatively small, allowing manufacturers to benefit directly from price differences. However, for server-side CPUs, the 2026 price adjustment serves as a &#8220;catch-up&#8221; for the 2025 consumer-grade price increases. Because contracts with major customers are typically semi-annual or annual, price adjustments require strong justification; consequently, incremental profit is partially offset by rising costs, though it still provides a positive contribution. From the demand side, Agentic AI is the primary reason for the growth in CPU demand. Read more",
    "source": "https://globalsemiresearch.substack.com/p/agentic-ai-next-cpu-inflection-point",
    "date": "2026-01-21T13:38:56.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-3",
    "content": "On January 12, DeepSeek released a paper introducing a new architecture called Engram, which separates &#8220;conditional memory&#8221; from &#8220;computation&#8221; in large language models, theoretically reducing errors and computational overhead. Our analysis reveals that Engram will likely accelerate infrastructure demand across compute, DRAM, and NAND. How Engram Actually Works After consulting with engineers specializing in this technology, we learned that Engram creates a two-track system: it saves computational resources when retrieving pre-stored knowledge, but cannot reduce compute power when generating new content. The system first queries a lookup table&#8212;think of it as checking whether similar knowledge or patterns have been previously stored. If a match is found, the model can directly retrieve this information via O(1) lookups rather than recomputing through neural networks. However, there are two critical limitations to the computational savings: Storage requirements remain unchanged. The overall memory footprint isn&#8217;t reduced because the system still needs to load the entire context&#8212;it simply skips computation for certain cached portions. Generative computation is unaffected. Once the model begins generating the next token, the KV cache for this pure computation phase cannot be optimized away. Investment Implications: Three Key Takeaways We&#8217;ve identified three counterintuitive implications for infrastructure investors: Read more",
    "source": "https://globalsemiresearch.substack.com/p/why-deepseeks-engram-is-positive",
    "date": "2026-01-18T11:41:08.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-4",
    "content": "Entering 2026, the global storage industry has witnessed a long-awaited robust recovery. The latest performance of Samsung and SK Hynix has directly boosted market enthusiasm, while the tight supply-demand pattern, iteration of new technologies, and transformation of the industry's business model have made this track full of highlights. This article discusses the core changes and future trends of the storage industry in 2026. I. Explosive Performance of Samsung &amp; SK Hynix, Full Growth Momentum First, let&#8217;s look at the most intuitive performance data. Both Samsung and SK Hynix achieved significant performance breakthroughs in Q4 2025. Samsung&#8217;s memory division posted an operating profit of 17 trillion Korean won, a year-on-year surge of 250%, with the average selling price (ASP) rising by over 40%, far exceeding market expectations; SK Hynix also delivered a strong performance in the same period, with revenue of 30.7 trillion Korean won, operating profit of 17.1 trillion Korean won, and an operating profit margin of nearly 56%. Among them, high-bandwidth memory (HBM) and high-density DDR5 products constituted the core growth drivers. This Substack is reader-supported. To receive new posts and support my work, consider becoming a free or paid subscriber. Behind the outstanding performance of the two companies is the support of the tight global storage market supply-demand pattern. Currently, global DRAM inventory is only 2-3 weeks, and NAND inventory is 3-4 weeks, both at historically low levels. This tight supply-demand situation is expected to last for a year, and suppliers will take a dominant position in price negotiations in the first half of 2026. On the demand side, in addition to stable server demand, the consumer electronics sector (mobile terminals, PCs) is expected to start inventory replenishment in Q2 2026, which will further consolidate the foundation for industry growth. II. Clear Price Uptrend The price trend is a core issue of concern ",
    "source": "https://globalsemiresearch.substack.com/p/2026-memory-industry-insights",
    "date": "2026-01-15T03:46:35.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-5",
    "content": "As the global leader in chip manufacturing, TSMC&#8217;s every move shapes the trajectory of the semiconductor industry. From the accelerated expansion of 3nm process to the mass production sprint of 2nm, coupled with adjustments to its overseas factory layout, TSMC&#8217;s strategic initiatives have far-reaching implications for the entire supply chain. Based on an expert interview published by Third Bridge, this article comprehensively deciphers TSMC&#8217;s capacity blueprint and future trends, offering an in-depth look into the high-stakes race for advanced process dominance. I. 3nm Process Node: Explosive Demand Growth Drives Beyond-Expectation Capacity Breakthroughs 1. Current Capacity Status: Targets Achieved Ahead of Schedule, Marching Toward 200,000 Wafers The capacity expansion progress of TSMC&#8217;s 3nm process has completely exceeded prior industry expectations. As early as the end of 2025, 3nm monthly capacity had already surpassed 150,000 wafers, hitting the preset target. As of 2026, expansion continues to accelerate, primarily driven by the continuous production ramp-up in the P7 and P8 phases of Fab 18B. Based on TSMC&#8217;s standard capacity of 25,000 wafers per phase, the full-scale operation of all 8 phases (P1&#8211;P8) at Fab 18B will deliver a monthly capacity of 200,000 wafers. Aligned with the current expansion pace and equipment installation progress, TSMC&#8217;s 3nm monthly capacity is highly likely to reach the 180,000&#8211;200,000 wafer range by the end of 2026, with potential to exceed this threshold. Behind this rapid capacity expansion lies explosive market demand, stemming mainly from two core segments. First, the sustained boom in the AI industry: as a leading AI chipmaker, NVIDIA has an urgent demand for 3nm capacity, with order volumes climbing steadily to become the backbone of 3nm demand. Second, the high-end consumer electronics and chip market: major industry players including Apple (for its iPhone 17 series and M-series ",
    "source": "https://globalsemiresearch.substack.com/p/decoding-tsmcs-advanced-process-roadmap",
    "date": "2026-01-13T04:09:41.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-6",
    "content": "At the start of 2026, NVIDIA officially announced the full-scale mass production of its next-generation Vera Rubin AI computing platform at the CES exhibition. The changes in liquid cooling are significant and have garnered widespread attention. This article analyzes the evolution of liquid cooling solutions, shifts in value volume, and the supply chain landscape. I. Upgrades to Liquid Cooling Solutions Faced with the surge in GPU power consumption from 1000W for the GB200 to 2.3kW for Rubin, NVIDIA has achieved a leapfrog improvement in heat dissipation capabilities through four core upgrades: 1. 100% Liquid Cooling Coverage Previously, the liquid cooling coverage of the GB200 was only 70%-80%, with critical components such as power supplies still relying on air cooling; the GB300 saw improvements but did not achieve full coverage. There&#8217;s a lesser-known fact about the GB200&#8217;s liquid cooling worth mentioning. I originally thought that all GB200 servers (Compute trays and switch trays) were liquid-cooled, but this is not the case. The NVL36 x 2 remained air-cooled for the most part, and it was only in the later NVL72 models that liquid cooling solutions were gradually introduced. Surprising, isn&#8217;t it? Because NVIDIA&#8217;s official images of the GB200 compute tray all showcase liquid cooling setups. The Rubin platform innovatively moves the power modules out of the integrated cabinet to create an independent POWER RACK, achieving 100% liquid cooling coverage for the first time. Combined with a cable-free, fanless modular design, it builds an integrated cabinet-level thermal management ecosystem characterized by &#8220;precision temperature control + high-efficiency heat exchange&#8221;. This breakthrough not only addresses heat dissipation blind spots for high-power chips but also significantly reduces cabinet air volume requirements (a reduction of 80% compared to the GB300), further improving energy efficiency. 2. Return and Upgrade of Cold Plat",
    "source": "https://globalsemiresearch.substack.com/p/nvidia-liquid-cooling-analysis-from",
    "date": "2026-01-09T10:49:17.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-7",
    "content": "CES 2026 will be held in Las Vegas from January 6 to 9, 2026. This year&#8217;s show is expected to redefine its focus: moving away from standalone consumer electronics to AI-driven system-level technological changes that are reshaping industries. In 2025, the tech market saw strong interest in cloud-based AI but moderate attention to edge-side applications. However, CES 2026 will be a key window to observe real-world AI implementations. Key areas include edge AI, industrial AI, Physical AI and automotive intelligence. Tech sector investments are also shifting. The focus is no longer just on computing infrastructure; instead, investors prioritize full technology cycles covering simulation, training and real-world deployment. Physical AI may become the core link connecting these different tech segments. We need to focus on four key areas: chips, autonomous driving, embodied intelligence, and XR. 1. Chips Nvidia originally planned to launch the RTX 50 SUPER series in Q1-Q2 2026, which may make its debut at CES 2026 or the company&#8217;s own press conference. AMD is likely to unveil new Ryzen chips (including Ryzen 7 9850X3D and Zen 5-based Ryzen 9000G) at its keynote; Intel will release the 2nm 18A-process Panther Lake (Core Ultra 3 series) for high-end laptops, with 50% boosts in both processing and Arc graphics performance; Qualcomm will focus on laptops, showcasing Snapdragon X2 Elite-equipped devices and launching the 18-core X2 Elite Extreme flagship. Over the past year, the global semiconductor industry has seen structural adjustments: cloud training computing power demand grows rapidly, while edge inference demand rises steadily. 2025&#8217;s AI PCs and phones were mostly marketing-driven; limited by video memory bandwidth and model compression bottlenecks, terminals still can&#8217;t run complex inference models smoothly, leading to a &#8220;booming cloud, lukewarm edge&#8221; bipolar market. The schedules of NVIDIA, AMD and Intel at CES 2026 are shown in the",
    "source": "https://globalsemiresearch.substack.com/p/ces-2026-preview-a-turning-point",
    "date": "2026-01-05T12:33:12.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-8",
    "content": "Once among China&#8217;s &#8220;Six Little Tigers&#8221; in AI, only 4 remain, with two currently pursuing listings on the Hong Kong Stock Exchange. At the end of 2025, Zhipu AI and Minimax simultaneously advanced their Hong Kong IPOs, kicking off a race for the &#8220;first domestic large-model stock,&#8221; becoming a key window to observe the differentiation of commercialization paths in the industry. While both operate in the core track, their paths are vastly different: Zhipu AI, with its Tsinghua academic roots, deeply cultivates B-end government and enterprise services, while Minimax, with its internet gene, focuses on C-end global expansion and multimodal innovation. This article will analyze the competitiveness and challenges of both from technical, commercial, and financial dimensions based on core data from their prospectuses, providing a reference for understanding the development of China&#8217;s large-model industry. Read more",
    "source": "https://globalsemiresearch.substack.com/p/who-is-the-most-capable-among-listed",
    "date": "2026-01-01T12:18:29.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-9",
    "content": "In 2025, the global AI computing power sector is undergoing unprecedented transformation and restructuring. In global markets, the focus of computing power competition has shifted from a mere race for chip performance to considerations of Total Cost of Ownership (TCO). The explosive growth in inference demand is reshaping the market landscape, while the vertical integration of sovereign computing power delivery models and the rise of sovereign computing power are adding further variables. Although NVIDIA remains dominant, the emergence of players such as AMD and Google&#8217;s TPU is quietly redrawing the boundaries of this &#8220;single superpower&#8221; monopoly. Meanwhile, driven by its unique industrial environment and policies, China&#8217;s AI computing power market has entered a period of explosive growth for domestic chips. The surge in IPOs, the rapid restructuring of the market landscape, and breakthroughs in supernode technology together outline a clear trajectory of domestic computing power moving from &#8220;catching up&#8221; to &#8220;running alongside&#8221; global leaders. In this computing power revolution, the enthusiasm of capital intertwines with the sobriety of technology, and debates about an AI bubble have become increasingly intense. This article will delve into the current status, trends, and challenges of AI computing power in 2025 from both global and Chinese perspectives, presenting a comprehensive panorama of this crucial field. U.S. Computing Power Status Aggressive Capex Looking back at 2023 and 2024, the main theme of the global AI computing power market was panic and hoarding. During those two years, whether it was Silicon Valley giants or sovereign funds, the only KPI was to get as many H100s as possible. According to UBS data, although the global AI chip market size is expected to exceed $200 billion in 2025, the driving force behind the growth has undergone a qualitative change. Now, cloud vendors, before placing orders, no longe",
    "source": "https://globalsemiresearch.substack.com/p/the-state-of-global-computing-power",
    "date": "2025-12-31T02:58:01.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-10",
    "content": "At the end of 2025, the competitive landscape of the AI industry has once again seen a pivotal shift&#8212;with the official release of Google&#8217;s flagship large model, Gemini 3 Pro . This not only refreshes the industry&#8217;s understanding of the upper limits of large models through core capabilities like full multimodality and ultra-long context, but also demonstrates a mature path for AI technology transitioning from the lab to scaled implementation via its complete ecosystem layout of &#8220;model family + application ecosystem + cloud support.&#8221; In this article, we will delve into Google&#8217;s advancement logic from three core dimensions: model technology breakthroughs, full-scenario ecosystem integration, and commercial monetization closed loop&#8212;while combining the current state of the Chinese market to analyze the industry insights it brings. This Substack is reader-supported. To receive new posts and support my work, consider becoming a free or paid subscriber. I. Comprehensive Evolution of the Model Family When discussing Google&#8217;s AI core strengths today, the unavoidable key label is the Gemini series of large models. Google&#8217;s latest flagship model, Gemini 3 Pro , builds on its predecessor&#8217;s foundations with enhanced multimodal reasoning, a 1-million-token context window, and superior performance across benchmarks. Google has not limited itself to iterating a single general-purpose large model but has instead built a &#8220;full-category coverage&#8221; model family strategy, forming a complete matrix from foundational general models to specialized models for niche scenarios. Beyond the core Gemini general large model series, this matrix includes several distinctive models: Veo 3.1 : Google&#8217;s latest video generation model, excelling in native audio-visual synchronization without post-processing. It supports multimodal inputs, outputs up to 1080p resolution, and generates base videos of 4&#8211;8 seconds (extendable ",
    "source": "https://globalsemiresearch.substack.com/p/deep-analysis-of-googles-ai-ecosystem",
    "date": "2025-12-30T03:35:36.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-11",
    "content": "TSMC&#8217;s CoWoS advanced packaging capacity is currently a matter of intense interest for major technology companies worldwide. We&#8217;ve been closely tracking these developments, and the following insights are drawn from an expert transcript shared by an investor in the semiconductor space. TSMC&#8217;s CoWoS Capacity Targets for 2026 As of late 2025, TSMC&#8217;s monthly CoWoS capacity stands at approximately 75&#8211;80 KWPM. The company is targeting a significant ramp-up, aiming for 120&#8211;130 KWPM by the end of 2026 through optimizations in existing facilities like AP8 and adjustments at AP7. Thanks for reading! Subscribe for free to receive new posts and support my work. Physical constraints, including limited land for new builds, mean TSMC is focusing on efficiency rather than entirely new dedicated CoWoS plants. Strategic Outsourcing in CoWoS Production TSMC plans to outsource a portion of its CoWoS workload in 2026, with estimates of 240,000&#8211;270,000 wafers annually handed off to OSAT partners&#8212;primarily Amkor (180,000&#8211;190,000 wafers) and SPIL (60,000&#8211;80,000 wafers). This shift is driven by multiple factors: Margin Optimization : TSMC prioritizes high-margin processes like silicon interposer and front-end CoW, outsourcing lower-margin substrate assembly and testing (requiring at least 50% gross margins for in-house work). Bottleneck Relief : Surging CoWoS-L demand has redirected internal capacity toward complex RDL interposers and silicon bridges, making outsourcing of CoWoS-S or substrate steps a logical move. Long-Term Transition : With CoPoS set for mass production by late 2027, heavy investment in round-wafer CoWoS lines risks future idling. Qualified OSATs like Amkor and SPIL, already certified for NVIDIA products and offering cost advantages, make outsourcing attractive. OSAT Margins in CoWoS Margins vary by scope: Substrate-only (OS): 35%&#8211;40% (standard flip-chip, lower value). Full CoW + OS: 40%&#8211;45% (higher d",
    "source": "https://globalsemiresearch.substack.com/p/tsmcs-cowos-capacity-scaling-up-outsourcing",
    "date": "2025-12-28T08:13:05.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-12",
    "content": "Cyclical Nature of the Memory Industry The memory industry exhibits distinct cyclical characteristics, having gone through multiple alternating ups and downs. In the first cycle from 2012-2015, the explosive growth of smartphones ignited demand and drove the industry upward, followed by a downturn due to declining PC sales and concentrated production expansion by manufacturers leading to oversupply. The second cycle from 2016-2019 saw upward momentum from Android phone storage capacity upgrades and DRAM supply shortages caused by manufacturers shifting to 3D NAND production, while the subsequent downturn was triggered by large-scale 3D NAND expansion and weak demand in PCs and servers. In the third cycle from 2020-2023, pandemic-induced growth in PC and server demand, along with 5G phone per-device storage upgrades, propelled the industry upward, but repeated pandemic disruptions led to sluggish consumer electronics terminal demand, pushing the industry into another downturn. Starting in 2024, the memory industry has entered a new cycle, with major manufacturers proactively reducing production to optimize supply structure, combined with the AI boom driving surging demand for high-end memory in servers and PCs, jointly propelling the industry into an upward channel. Thanks for reading! Subscribe for free to receive new posts and support my work. Logic Behind This Round of Memory Surge DRAM Price Surge This round of memory surge began with the explosive rise in DDR4 prices mid-year, resulting from significant supply contraction, resilient demand, market panic, and delays in industry technology iteration. On the supply side, in 2025, leading manufacturers like Micron and Samsung announced the upcoming end-of-life (EOL) for DDR4, with last-time buy fulfillment rates far below expectations, severely shaking long-term supply confidence. More critically, under the AI boom, HBM and DDR5 became priority directions due to higher profits and suitability for high-end servers. S",
    "source": "https://globalsemiresearch.substack.com/p/the-ai-driven-memory-supercycle-surge",
    "date": "2025-12-23T03:28:40.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-13",
    "content": "Recent reports of Meta placing orders for Google&#8217;s TPU have generated considerable buzz in the industry. To better understand this development, let&#8217;s examine an expert interview transcript published by AceCamp on November 28, which delves into Meta&#8217;s rationale, the structure of the chip deal, and whether Meta is renting from Google or purchasing the chips directly. Let&#8217;s dive in. Why Meta is Ordering TPUs According to the expert, Meta&#8217;s decision to order TPUs stems primarily from the compatibility between its business needs and TPU capabilities. The Meta-Google collaboration operates on two tracks: First, Meta will rent Google&#8217;s TPUs in 2026 to test its Llama models, experimenting with both inference and training on TPU infrastructure to evaluate the practicality of porting their workloads. If the TPUs can adequately support Llama&#8217;s training and inference requirements, Meta plans to purchase TPUs from Google in 2027 and deploy them in its own data centers. The business logic behind this collaboration is straightforward. Meta shares certain model-level characteristics with Anthropic, and both Meta and Google&#8217;s Gemini models employ similar Mixture-of-Experts (MoE) architectures at the foundational level. Google&#8217;s Gemini model has already demonstrated strong returns on GPU deployments, delivering clear value in both training and inference. Given this track record, Meta wants to evaluate whether Google&#8217;s TPUs can deliver superior value compared to NVIDIA GPUs when deployed at scale. This potential for improved ROI represents the core commercial driver behind the partnership. Renting vs. Purchasing When asked whether Meta would ultimately rent or purchase the chips, the expert indicated that Meta&#8217;s long-term vision centers on chip procurement rather than rental. Google&#8217;s TPUs already support both TensorFlow and PyTorch&#8212;the latter being developed by Meta itself&#8212;meaning many of Meta&#8217;s",
    "source": "https://globalsemiresearch.substack.com/p/inside-metas-tpu-order-what-the-google",
    "date": "2025-12-03T13:42:14.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-14",
    "content": "With Gemini 3&#8217;s release, the gap between Chinese AI models and their U.S. counterparts is expanding once again. The most impressive capability Gemini 3 has demonstrated is its multimodal prowess. In this area, Chinese large language models lag behind the U.S. by approximately half a generation (1-3 months). Google&#8217;s U.S. competitors are expected to catch up to Gemini 3&#8217;s multimodal capabilities within 2-3 months (by January), while leading Chinese companies are projected to take an additional 1-2 months (roughly 4 months total). Before examining each major Chinese AI lab&#8217;s key challenges and strategic focus in AI model development, let&#8217;s first review Gemini 3&#8217;s technological breakthroughs and impact. Gemini 3&#8217;s Technical Advantages Gemini 3 has established significant gaps with competitors and previous generations across five core areas: multimodal training, reasoning capabilities, tool integration, ultra-long context handling, and data training optimization. This technical advantage is expected to last 2-3 months through early next year. Multimodal Training Innovation : Gemini 3 employs a unified encoder as its core pre-training architecture, performing native end-to-end encoding of text, images, video, and audio from the initial training phase. This replaces the traditional approach of separate modal encoding followed by late-stage fusion. Combined with YouTube&#8217;s vast video dataset, it designs spatiotemporal pre-training alignment tasks that significantly enhance core multimodal capabilities. Enhanced Reasoning : Rather than treating reasoning as merely a fine-tuning task, Gemini 3 integrates reasoning capabilities into its core pre-training objectives. It constructs massive-scale (over 100 billion tokens) dynamic deep cognitive training datasets covering mathematical proofs, scientific experimental procedures, and code debugging steps, achieving foundational model reinforcement during early pre-training. Optimized T",
    "source": "https://globalsemiresearch.substack.com/p/chinas-ai-gap-with-the-us-widens",
    "date": "2025-11-24T12:31:05.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-15",
    "content": "It has become a widely accepted fact in the global AI community that Chinese AI labs can develop high-performing AI models using fewer chips than their U.S. counterparts. Some Chinese models even surpass American ones on certain benchmarks. The recent success of Kimi&#8217;s K2 thinking serves as a prime example, not to mention DeepSeek&#8217;s broader achievements. However, when it comes to return on AI investments&#8212;how AI spending translates into actual revenue&#8212;China appears to lag behind its U.S. peers. To illustrate this gap, we compared three major Chinese tech companies (ByteDance, Alibaba, and Tencent) with their American counterparts (Microsoft, Google, and Amazon). Methodology Our analysis calculates the return on AI investment by dividing each company&#8217;s estimated 2025 AI-related revenue (primarily cloud services and external-facing revenue streams) by their 2025 capital expenditure (using market consensus estimates). Key Details: ByteDance generates approximately 6 trillion daily tokens externally (about 75% of Alibaba&#8217;s volume). Based on Alibaba&#8217;s pricing model, this translates to roughly $1.1B in AI PaaS value. Additionally, ByteDance&#8217;s Volcano AI platform captures an estimated 20% market share in China&#8217;s GenAI-as-a-Service market, contributing approximately $1.4B in compute revenue. Total AI revenue: $2.6B Alibaba derives AI revenue from two main streams: $1.7B from AI compute services and $1.6B from AI PaaS offerings, representing a 25% market share in China&#8217;s rapidly growing GenAI market. Total AI revenue: $3.3B Tencent&#8217;s AI revenue encompasses multiple services including GPU PaaS, RAG solutions, ADP, CodeBuddy, and traditional AI functionalities. According to expert interviews, these combined AI services are projected to generate Total AI revenue: $0.9B in 2025. Microsoft&#8217;s AI services (including model APIs, Copilot, and AI platforms) now represent an estimated 12% of Azure&#8217;s revenue. W",
    "source": "https://globalsemiresearch.substack.com/p/chinese-ai-labs-excel-in-efficiency",
    "date": "2025-11-20T04:40:14.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-16",
    "content": "We previously covered Amazon&#8217;s ASIC project in an earlier piece , updating readers on its latest developments. Today, we turn our attention to Google&#8217;s TPU (Tensor Processing Unit), which many consider the most significant threat to Nvidia&#8217;s dominance in AI computing. In this piece, we&#8217;ll examine Google&#8217;s TPU shipments, revenue projections, and product mix for 2025, along with our outlook for 2026. We&#8217;ll also provide updates on the company&#8217;s V7 chip development. This analysis is based on an expert interview with an industry insider from a company with substantial ASIC operations. Readers should note that this data is for reference purposes only. A key takeaway is that TPU operations have arguably become one of Google&#8217;s most important revenue streams and will only grow in significance, potentially posing a major challenge to Nvidia&#8217;s market position. 2025 Shipments, Revenue, and Product Mix According to the expert source, Google&#8217;s TPU shipments are projected to reach 2.5 million units for the full year 2025. Through the third quarter of 2025, cumulative shipments have already reached 1.8 million units, representing 72% of the annual target. Breaking down the quarterly distribution: Q1 shipments totaled approximately 500,000 units, providing a steady start to the year. Q2 saw modest growth to around 550,000 units, maintaining an upward trajectory. Q3 witnessed significant acceleration with 700,000-750,000 units shipped, coinciding with increased data center deployment activity in the second half. Q4 must deliver the remaining 700,000-800,000 units to meet the 2.5 million annual target. Within the 2.5 million total shipments for 2025, different models show distinct volume patterns. The TPU V5 series (encompassing V5E and V5P) serves as the current flagship product line, with projected shipments of 1.9 million units representing 76% of total volume. Specifically, V5E accounts for approximately 1.2 million units",
    "source": "https://globalsemiresearch.substack.com/p/googles-tpu-challenge-to-nvidia-a",
    "date": "2025-11-09T09:34:20.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-17",
    "content": "The AI boom has triggered an unprecedented data center construction frenzy, and copper has emerged as one of the biggest beneficiaries. As companies race to build the infrastructure needed for artificial intelligence, demand for this essential metal has surged dramatically. Copper plays four critical roles in AI data center development. It enables rapid signal transmission between chips and systems, ensuring the lightning-fast data transfer that AI applications require. The metal also provides reliable power delivery through efficient electrical distribution networks. Its exceptional thermal conductivity makes it indispensable for cooling systems that prevent overheating in high-performance computing environments. Finally, copper serves as a vital component in the sophisticated semiconductor manufacturing process itself. For this piece, we will examine China&#8217;s pivotal position in the global copper market, both as a producer and consumer, with particular focus on how data center expansion is driving unprecedented demand. Copper supply and demand On the supply side, copper faces structural challenges. Global reserves are growing slowly and remain concentrated in a handful of major ore belts. While China possesses relatively abundant copper reserves, the gap between domestic reserves and production output has widened significantly in recent years. The quality of global copper ore continues to deteriorate, with substantial regional variations. North American copper reserves average below 2.5% grade, with some deposits as low as 0.29%. African reserves show higher grades, ranging from 0.45% to 5.8%, making them increasingly attractive to international investors. Investment in copper exploration has been declining gradually, though Latin America continues to attract a relatively large share of exploration capital. Chinese companies have responded by increasing their overseas investments, particularly targeting copper mines in Africa and South America. Despite modest",
    "source": "https://globalsemiresearch.substack.com/p/the-big-demand-of-copper-in-ai-supply",
    "date": "2025-11-04T07:07:04.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-18",
    "content": "Amazon&#8217;s spectacular third-quarter earnings , particularly the 20% growth rate for its AWS service reported this week, have eased fears that the company may struggle to catch up with peers Microsoft and Google in cloud growth. However, investors may be overly optimistic about Amazon&#8217;s prospects, as it remains unclear whether this 20% growth is driven by GPU services or traditional CPU servers. In recent months, Amazon has been aggressively promoting its internally developed chips as the backbone of its AI cloud computing services. The company has touted that its Trainium chips deliver the same performance as Nvidia&#8217;s offerings at a significantly lower cost. According to a March report by The Information, AWS has been pitching clients on renting Trainium-powered servers that would provide computing power equivalent to Nvidia&#8217;s H100 chips at just 25% of the price. Trainium&#8217;s success will be critical not only to Amazon&#8217;s ability to compete in the GPU rental business&#8212;a battle it cannot afford to lose given that this represents the future of cloud computing growth&#8212;but also to the company&#8217;s broader AI strategy. Amazon has bet heavily on using Trainium as the foundation for its entire AI portfolio, from model offerings to SaaS services. For today&#8217;s analysis, we&#8217;ll examine an expert transcript published by research firm AceCamp last month to assess how Trainium is performing. The interviewee is a business manager at Synopsys. A key takeaway from this interview is that Trainium has yet to win over significant clients, suggesting Amazon still has considerable ground to cover before making Trainium a cornerstone of its cloud business&#8212;and before it can reduce its dependence on purchasing Nvidia&#8217;s chips. The following is an excerpt from the interview: Question: Regarding the AWS Trainium project, please provide detailed information about its development history, including launch timelines, functional c",
    "source": "https://globalsemiresearch.substack.com/p/amazons-trainium-chips-are-still",
    "date": "2025-11-02T02:36:47.000Z",
    "platform": "substack"
  },
  {
    "id": "substack-19",
    "content": "On Sunday evening, news that shocked China&#8217;s entire semiconductor industry broke: Wingtech announced that the Dutch government had issued a ministerial order freezing all of Nexperia&#8217;s assets, intellectual property, operations, and personnel changes for one year&#8212;effectively seizing control of the semiconductor company. The move has sparked outrage among industry players and investors, with many labeling the Dutch action as &#8220;robbery.&#8221; This incident represents yet another casualty of the escalating China-U.S. technology rivalry and reveals a sobering reality: the semiconductor industry, inherently global in nature, faces an unprecedented test as decades of rule-based, trust-driven cooperation models are now being overridden by national security concerns. The case serves as a textbook warning for any Chinese company attempting to acquire core technology through overseas acquisitions, demonstrating that such deals are no longer viable in the current geopolitical climate. During an investor call on Sunday, Wingtech explained that the Dutch government has effectively pressed the &#8220;pause button&#8221; on Nexperia, with courts appointing a temporary administrator to manage the company alongside foreign executives. While Wingtech&#8217;s operational and management rights have been suspended, the company emphasized that shareholder economic interests remain unchanged, with 100% of Nexperia&#8217;s profits still belonging to the listed company. Before analyzing the broader implications, lets first take a look on this incident&#8217;s origins, which appear to be part of a carefully orchestrated plan rather than a sudden decision. Wingtech&#8217;s acquisition of Nexperia&#8212;spun off from semiconductor giant NXP&#8212;for 33.2 billion yuan in 2018 was widely viewed as a case of &#8220;snake swallowing an elephant.&#8221; At the time, Wingtech was China&#8217;s largest smartphone ODM, a business it has since divested. The deal&#8217;s driving ",
    "source": "https://globalsemiresearch.substack.com/p/nexperia-seizure-impact-on-wingtech",
    "date": "2025-10-13T07:15:19.000Z",
    "platform": "substack"
  }
]